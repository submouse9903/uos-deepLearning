{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/submouse9903/uos-deepLearning/blob/main/U47768_CH06_Transformer(Attention).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8QLh10r8OD0"
      },
      "outputs": [],
      "source": [
        "## torch version 2.0\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### - Seq2Seq 에서 Q, K, V 의 역할\n",
        "- query: output seq \n",
        "- key: input seq\n",
        "- value: input seq\n",
        "\n",
        "Attention은 embedded input sequence ($X_V$)를 output sequence ($X_Q$)와 input sequence ($X_K$), input seq의 다른 표현형)의 유사도를 이용하여 새로운 embedded input sequence를 출력한다.\n",
        "\n",
        "- $X_Q \\in \\mathbb{R}^{S \\times d_e} $: 길이가 $L$, 각 원소가 $d_e$차원에 embedding (내재화)됨.\n",
        "- $X_K \\in \\mathbb{R}^{S \\times d_k} $: 길이가 $S$, 각 원소가 $d_k$차원에 embedding (내재화)됨.\n",
        "- $X_V \\in \\mathbb{R}^{S \\times d_v} $: 길이가 $S$, 각 원소가 $d_v$차원에 embedding (내재화)됨.\n",
        "\n",
        "\n",
        "다음을 만족한다. \n",
        "- $Q = X_Q W_Q^\\top \\in \\mathbb{R}^{S \\times d_e}$, 여기서 $W_Q \\in \\mathbb{R}^{d_e \\times d_e}$\n",
        "- $K = X_K W_K^\\top \\in \\mathbb{R}^{S \\times d_e}$, 여기서  $W_K \\in \\mathbb{R}^{d_e \\times d_k}$\n",
        "- $V = X_V W_V^\\top \\in \\mathbb{R}^{S \\times d_e}$, 여기서  $W_K \\in \\mathbb{R}^{d_e \\times d_k}$\n",
        "\n",
        "Attention weight 는 다음 값의 softmax  값으로 계산된다. \n",
        "$Q K^\\top  =  X_Q W_Q^\\top W_K X_K \\in \\mathbb{R}^{L \\times S} $\n",
        "\n",
        "Attention의 출력은 \n",
        "$$ \\mbox{softmax}(Q K^\\top/\\sqrt{d})V \\in \\mathbb{R}^{L \\times d_e} $$\n",
        "가 된다. \n",
        "\n",
        "\n",
        "**Attention 결론**\n",
        "\n",
        "*Attention은 ''$d_v$ 차원에 embedding 된 길이 $S$의 input sequence'' 를 ''$d_e$ 차원에 embedding 된 길이 $L$의 output sequence'' 로 변환하는 seq2seq-layer다*\n"
      ],
      "metadata": {
        "id": "lkHOiyNSa5bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 변환후 feature dim\n",
        "embed_dim = 4\n",
        "# attention weight 를 계산할 input이 표현된 feature 차원\n",
        "kdim = 7\n",
        "# 현재 input이 표현되어 있는 feature의 차원  \n",
        "vdim = 5\n",
        "\n",
        "num_heads = 1\n",
        "multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True,\n",
        "                                       kdim=kdim, vdim=vdim)"
      ],
      "metadata": {
        "id": "ENopmaJ3avpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MultiheadAttention 의 경우 Attention 연산 후에 out proj 연산이 있다. 이 연산은 $d_e \\times d_e$ 행렬에 의해 이루어진다."
      ],
      "metadata": {
        "id": "YpsC9vdMCNBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in multihead_attn.named_parameters():\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smV8aVnR7PcM",
        "outputId": "2a7f5e3a-4f5c-40ab-9e7b-2dcf3f555ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('q_proj_weight', Parameter containing:\n",
            "tensor([[ 0.8081,  0.8300,  0.3251,  0.6588],\n",
            "        [ 0.7569, -0.8277,  0.2550,  0.0611],\n",
            "        [-0.4685,  0.6590,  0.2442,  0.3335],\n",
            "        [ 0.1542, -0.0311,  0.4335,  0.0395]], requires_grad=True))\n",
            "('k_proj_weight', Parameter containing:\n",
            "tensor([[ 0.0460, -0.3858,  0.3785, -0.3876, -0.3238,  0.3952, -0.2606],\n",
            "        [-0.0296,  0.2311, -0.5357, -0.5317,  0.4605, -0.1561, -0.5099],\n",
            "        [-0.3090,  0.3169,  0.5062,  0.0731, -0.7035,  0.6866, -0.6652],\n",
            "        [ 0.3952,  0.5441, -0.7128,  0.0107, -0.0311, -0.0951,  0.0376]],\n",
            "       requires_grad=True))\n",
            "('v_proj_weight', Parameter containing:\n",
            "tensor([[ 0.6255, -0.0111, -0.0438,  0.5052, -0.3811],\n",
            "        [-0.6044,  0.1516, -0.2924, -0.0771,  0.3047],\n",
            "        [ 0.6424, -0.8096,  0.0712,  0.6121,  0.5133],\n",
            "        [ 0.1129,  0.2214, -0.1532,  0.3119,  0.6485]], requires_grad=True))\n",
            "('in_proj_bias', Parameter containing:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True))\n",
            "('out_proj.weight', Parameter containing:\n",
            "tensor([[ 0.3965, -0.0984, -0.0811, -0.2227],\n",
            "        [-0.3909, -0.2235, -0.2142,  0.1912],\n",
            "        [-0.1268,  0.0384,  0.2480,  0.2762],\n",
            "        [ 0.2688,  0.2479, -0.1247, -0.4189]], requires_grad=True))\n",
            "('out_proj.bias', Parameter containing:\n",
            "tensor([0., 0., 0., 0.], requires_grad=True))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of samples\n",
        "n = 2\n",
        "# output seq의 길이\n",
        "L = 3\n",
        "# input seq의 길이\n",
        "S = 2\n",
        "# data\n",
        "X_q = torch.rand(n,L,embed_dim)\n",
        "X_k = torch.rand(n,S,kdim)\n",
        "X_v = torch.rand(n,S,vdim)"
      ],
      "metadata": {
        "id": "d2-pe9_Nwigt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output, attn_output_weights = multihead_attn(X_q, X_k, X_v)"
      ],
      "metadata": {
        "id": "eWB16klsNsxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output_weights"
      ],
      "metadata": {
        "id": "79IunZ9WxLu0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1589749-5a9c-48ee-e2fa-e5c65f2eeb07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.3921, 0.6079],\n",
              "         [0.4287, 0.5713],\n",
              "         [0.4340, 0.5660]],\n",
              "\n",
              "        [[0.4989, 0.5011],\n",
              "         [0.4722, 0.5278],\n",
              "         [0.4834, 0.5166]]], grad_fn=<MeanBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- attn_output_weights 의 설명\n",
        "  - input seq를  output seq 로 변환할 때, 각 output 변환값이 input 값의 어떤 구성요소로 이루어져있는지 비율로 표현\n",
        "  - 현재는 input seq 의 길이가 2고 output seq의 길이가 3\n",
        "  - 각 행은 output seq를 나타내며 행벡터의 원소값은 input seq 원소들의 가중치를 나타낸다. \n",
        "\n"
      ],
      "metadata": {
        "id": "880cYHhq_WK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKPpKvVnxQiT",
        "outputId": "9a0ca9e9-0779-42e3-9739-8523516ae13d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.1778,  0.0810,  0.2770, -0.3631],\n",
              "         [-0.1738,  0.0801,  0.2782, -0.3670],\n",
              "         [-0.1732,  0.0800,  0.2784, -0.3676]],\n",
              "\n",
              "        [[ 0.0844, -0.2254,  0.2697, -0.2622],\n",
              "         [ 0.0825, -0.2279,  0.2751, -0.2666],\n",
              "         [ 0.0833, -0.2268,  0.2728, -0.2648]]], grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3행 4열 output을 얻게 되었는데, 이는 output length가 3이며 각 원소가 4차원에 embedding 된 것이다. "
      ],
      "metadata": {
        "id": "cAc_J1Onxaoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Masking 의 활용\n",
        "\n",
        "- (L,S) 의 행렬이 필요함\n"
      ],
      "metadata": {
        "id": "W1cbQwv_AogO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_mat = np.array([[1,-100], [1,-100],[1,1]])\n",
        "mask_mat = torch.tensor(mask_mat, dtype = torch.float)"
      ],
      "metadata": {
        "id": "A2IkUUvMAu95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output, attn_output_weights = multihead_attn(X_q, X_k, X_v,\n",
        "                                                  attn_mask = mask_mat)"
      ],
      "metadata": {
        "id": "YZsBkyyvFYCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BSvpQr1FfBO",
        "outputId": "76d321d2-a12c-4c43-dc6a-e74339a4eef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.0000e+00, 2.1019e-44],\n",
              "         [1.0000e+00, 1.8217e-44],\n",
              "         [4.3401e-01, 5.6599e-01]],\n",
              "\n",
              "        [[1.0000e+00, 1.4013e-44],\n",
              "         [1.0000e+00, 1.5414e-44],\n",
              "         [4.8345e-01, 5.1655e-01]]], grad_fn=<MeanBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multihead Attention\n"
      ],
      "metadata": {
        "id": "jlzs4nDMG4L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_heads = 4\n",
        "multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True,\n",
        "                                       kdim=kdim, vdim=vdim)"
      ],
      "metadata": {
        "id": "VH9Fdzu5G8lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,j in multihead_attn.named_parameters():\n",
        "  print(i)\n",
        "  print(j)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNjPoxlcHntR",
        "outputId": "68b25917-1c89-4b29-fc57-f47c0e50fe17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q_proj_weight\n",
            "Parameter containing:\n",
            "tensor([[ 0.6570,  0.5942, -0.4123,  0.6315],\n",
            "        [-0.1148,  0.4508, -0.2153, -0.6365],\n",
            "        [ 0.3044, -0.7152, -0.2825,  0.0821],\n",
            "        [-0.6130,  0.8284,  0.1979, -0.2382]], requires_grad=True)\n",
            "k_proj_weight\n",
            "Parameter containing:\n",
            "tensor([[ 0.1517,  0.1926, -0.5911,  0.4140, -0.3239, -0.2350, -0.2235],\n",
            "        [ 0.0214, -0.4754, -0.4389, -0.1492,  0.4507, -0.2647, -0.1339],\n",
            "        [ 0.2932, -0.1135, -0.1349, -0.0822, -0.6823, -0.3712,  0.3190],\n",
            "        [ 0.0010,  0.1696,  0.2470, -0.6032,  0.3760, -0.6372, -0.6300]],\n",
            "       requires_grad=True)\n",
            "v_proj_weight\n",
            "Parameter containing:\n",
            "tensor([[-0.6550, -0.4997,  0.4274,  0.6898, -0.0595],\n",
            "        [-0.3655,  0.6069, -0.3514, -0.7861, -0.2132],\n",
            "        [-0.2290,  0.6656,  0.4749, -0.3340, -0.2303],\n",
            "        [ 0.4406, -0.1715,  0.4626,  0.5878,  0.3684]], requires_grad=True)\n",
            "in_proj_bias\n",
            "Parameter containing:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
            "out_proj.weight\n",
            "Parameter containing:\n",
            "tensor([[ 0.3180, -0.1202,  0.3198,  0.4232],\n",
            "        [ 0.2848, -0.3101, -0.0058, -0.0723],\n",
            "        [ 0.3693, -0.4837,  0.4009, -0.1057],\n",
            "        [-0.1533, -0.2210,  0.2539,  0.3563]], requires_grad=True)\n",
            "out_proj.bias\n",
            "Parameter containing:\n",
            "tensor([0., 0., 0., 0.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### - Self Attention"
      ],
      "metadata": {
        "id": "S8oTsEO9Hl6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 변환후 feature dim\n",
        "embed_dim = 4\n",
        "num_heads = 1\n",
        "multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)"
      ],
      "metadata": {
        "id": "fa3JXMFIItth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of samples\n",
        "n = 2\n",
        "# input seq의 길이\n",
        "S = 5\n",
        "# data\n",
        "X_v = torch.rand(n,S,embed_dim)"
      ],
      "metadata": {
        "id": "5OzhHpx4lRiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output, weight = multihead_attn(X_v, X_v, X_v)"
      ],
      "metadata": {
        "id": "hEJVhh-zi_V2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"#### output of self attention\")\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOpevbdoltFH",
        "outputId": "c3c177f1-b287-4a71-e41d-9690e2c8898a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### output of self attention\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 5, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}